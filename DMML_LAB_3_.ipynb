{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOVZFHH4/Dar9M51b6YOL3B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arunkumarvp/Data-mining-and-machine-learning-/blob/main/DMML_LAB_3_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***24CY731** - **DATA MINING AND MACHINE LERANING IN CYBER SECURITY***\n",
        "\n",
        "#LAB-3\n",
        "\n",
        "Arun Kumar P\n",
        "CB.SC.P2CYS25033"
      ],
      "metadata": {
        "id": "zlVIWGH7S_UR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1) Use StandardScaler to standardize the features of a Credit card fraud dataset Include code, description and screenshots of outputs."
      ],
      "metadata": {
        "id": "iqCFy8DRYDH3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sklearn.preprocessing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "#import dataset from kaggle\n",
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "# Set the path to the file you'd like to load\n",
        "\n",
        "file_path = \"creditcard.csv\"\n",
        "\n",
        "# Load the latest version\n",
        "df = kagglehub.load_dataset(\n",
        "  KaggleDatasetAdapter.PANDAS,\n",
        "  \"mlg-ulb/creditcardfraud\",\n",
        "  file_path,\n",
        ")\n",
        "\n",
        "print(\"Data Sample: \" , df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "8FPejogyXxvo",
        "outputId": "76b9584f-e32a-4b71-c76d-9d8d9819e30c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4143267421.py:17: DeprecationWarning: Use dataset_load() instead of load_dataset(). load_dataset() will be removed in a future version.\n",
            "  df = kagglehub.load_dataset(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'creditcardfraud' dataset.\n",
            "Data Sample:     Time        V1        V2        V3  ...       V27       V28  Amount  Class\n",
            "0   0.0 -1.359807 -0.072781  2.536347  ...  0.133558 -0.021053  149.62      0\n",
            "1   0.0  1.191857  0.266151  0.166480  ... -0.008983  0.014724    2.69      0\n",
            "2   1.0 -1.358354 -1.340163  1.773209  ... -0.055353 -0.059752  378.66      0\n",
            "3   1.0 -0.966272 -0.185226  1.792993  ...  0.062723  0.061458  123.50      0\n",
            "4   2.0 -1.158233  0.877737  1.548718  ...  0.219422  0.215153   69.99      0\n",
            "\n",
            "[5 rows x 31 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de2eace0"
      },
      "source": [
        "# Task\n",
        "Standardize the 'Time' and 'Amount' columns in the `df` DataFrame using `StandardScaler`, replacing the original columns with their scaled versions, then display the first few rows of the modified DataFrame and its descriptive statistics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "896cb898"
      },
      "source": [
        "## Standardize 'Time' and 'Amount'\n",
        "\n",
        "### Subtask:\n",
        "Apply the StandardScaler to the 'Time' and 'Amount' columns of the DataFrame to standardize these features. The original columns will be replaced with their scaled versions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5381be8"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires standardizing the 'Time' and 'Amount' columns using StandardScaler. I will initialize StandardScaler and then apply fit_transform to each column, overwriting the original columns with the scaled values.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "5920ac6d",
        "outputId": "fd59e743-5e16-4172-ee8f-d2bfc34a00f5"
      },
      "source": [
        "scaler = StandardScaler()\n",
        "df['Time'] = scaler.fit_transform(df[['Time']])\n",
        "df['Amount'] = scaler.fit_transform(df[['Amount']])\n",
        "print(\"DataFrame after standardizing 'Time' and 'Amount' columns:\")\n",
        "print(df[['Time', 'Amount']].head())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame after standardizing 'Time' and 'Amount' columns:\n",
            "       Time    Amount\n",
            "0 -1.996583  0.244964\n",
            "1 -1.996583 -0.342475\n",
            "2 -1.996562  1.160686\n",
            "3 -1.996562  0.140534\n",
            "4 -1.996541 -0.073403\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30f46ca7"
      },
      "source": [
        "**Reasoning**:\n",
        "The 'Time' and 'Amount' columns have been standardized. To confirm this and inspect the modified DataFrame, I will display the first few rows of the entire DataFrame and its descriptive statistics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "956f5dff",
        "outputId": "4772b0c5-9fc9-473e-b479-5f62b5f11652"
      },
      "source": [
        "print(\"Modified DataFrame head:\")\n",
        "print(df.head())\n",
        "print(\"\\nDescriptive statistics of the modified DataFrame:\")\n",
        "print(df.describe())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modified DataFrame head:\n",
            "       Time        V1        V2        V3  ...       V27       V28    Amount  Class\n",
            "0 -1.996583 -1.359807 -0.072781  2.536347  ...  0.133558 -0.021053  0.244964      0\n",
            "1 -1.996583  1.191857  0.266151  0.166480  ... -0.008983  0.014724 -0.342475      0\n",
            "2 -1.996562 -1.358354 -1.340163  1.773209  ... -0.055353 -0.059752  1.160686      0\n",
            "3 -1.996562 -0.966272 -0.185226  1.792993  ...  0.062723  0.061458  0.140534      0\n",
            "4 -1.996541 -1.158233  0.877737  1.548718  ...  0.219422  0.215153 -0.073403      0\n",
            "\n",
            "[5 rows x 31 columns]\n",
            "\n",
            "Descriptive statistics of the modified DataFrame:\n",
            "               Time            V1  ...        Amount          Class\n",
            "count  2.848070e+05  2.848070e+05  ...  2.848070e+05  284807.000000\n",
            "mean  -3.065637e-16  1.168375e-15  ...  2.913952e-17       0.001727\n",
            "std    1.000002e+00  1.958696e+00  ...  1.000002e+00       0.041527\n",
            "min   -1.996583e+00 -5.640751e+01  ... -3.532294e-01       0.000000\n",
            "25%   -8.552120e-01 -9.203734e-01  ... -3.308401e-01       0.000000\n",
            "50%   -2.131453e-01  1.810880e-02  ... -2.652715e-01       0.000000\n",
            "75%    9.372174e-01  1.315642e+00  ... -4.471707e-02       0.000000\n",
            "max    1.642058e+00  2.454930e+00  ...  1.023622e+02       1.000000\n",
            "\n",
            "[8 rows x 31 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) Apply SMOTE to address the class imbalance problem. Include code, description and screenshots of outputs."
      ],
      "metadata": {
        "id": "EtTrHocxcrXa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91a7a134"
      },
      "source": [
        "## Apply SMOTE for Class Imbalance\n",
        "\n",
        "### Subtask:\n",
        "Apply SMOTE (Synthetic Minority Over-sampling Technique) to the dataset to address the class imbalance problem. This involves separating the features and target variable, then using SMOTE to create synthetic samples for the minority class, and finally, examining the new class distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57e7e749"
      },
      "source": [
        "**Reasoning**:\n",
        "SMOTE (Synthetic Minority Over-sampling Technique) is an oversampling method that generates synthetic samples from the minority class. This helps to balance the class distribution, which is crucial for training machine learning models on imbalanced datasets, as it prevents the model from being biased towards the majority class. I will first import the necessary `SMOTE` class and then prepare the data for application."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "318c7ca2",
        "outputId": "38dc7832-b6c5-4800-b7ea-d54e3f61f6dd"
      },
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = df.drop('Class', axis=1)\n",
        "y = df['Class']\n",
        "\n",
        "# Display the original class distribution\n",
        "print(\"Original class distribution:\")\n",
        "print(y.value_counts())\n",
        "\n",
        "# Initialize SMOTE\n",
        "sm = SMOTE(random_state=42)\n",
        "\n",
        "# Apply SMOTE to the dataset\n",
        "X_res, y_res = sm.fit_resample(X, y)\n",
        "\n",
        "# Display the new class distribution\n",
        "print(\"\\nNew class distribution after SMOTE:\")\n",
        "print(y_res.value_counts())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original class distribution:\n",
            "Class\n",
            "0    284315\n",
            "1       492\n",
            "Name: count, dtype: int64\n",
            "\n",
            "New class distribution after SMOTE:\n",
            "Class\n",
            "0    284315\n",
            "1    284315\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fe8fc6f"
      },
      "source": [
        "**Reasoning**:\n",
        "The SMOTE operation has been performed, and the class distribution has been displayed. To provide a clear output and address the user's request for documentation, I will combine the resampled features and target back into a new DataFrame and display its head, along with a description of the class distributions for comparison."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "068f8dda",
        "outputId": "96a5ca87-aca9-4984-8eca-575718367d2f"
      },
      "source": [
        "# Combine the resampled features and target into a new DataFrame\n",
        "df_resampled = pd.concat([X_res, y_res], axis=1)\n",
        "\n",
        "print(\"\\nDataFrame head after SMOTE:\")\n",
        "print(df_resampled.head())\n",
        "\n",
        "print(\"\\nDescriptive statistics of the resampled DataFrame:\")\n",
        "print(df_resampled.describe())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DataFrame head after SMOTE:\n",
            "       Time        V1        V2        V3  ...       V27       V28    Amount  Class\n",
            "0 -1.996583 -1.359807 -0.072781  2.536347  ...  0.133558 -0.021053  0.244964      0\n",
            "1 -1.996583  1.191857  0.266151  0.166480  ... -0.008983  0.014724 -0.342475      0\n",
            "2 -1.996562 -1.358354 -1.340163  1.773209  ... -0.055353 -0.059752  1.160686      0\n",
            "3 -1.996562 -0.966272 -0.185226  1.792993  ...  0.062723  0.061458  0.140534      0\n",
            "4 -1.996541 -1.158233  0.877737  1.548718  ...  0.219422  0.215153 -0.073403      0\n",
            "\n",
            "[5 rows x 31 columns]\n",
            "\n",
            "Descriptive statistics of the resampled DataFrame:\n",
            "                Time             V1  ...         Amount     Class\n",
            "count  568630.000000  568630.000000  ...  568630.000000  568630.0\n",
            "mean       -0.159338      -2.315118  ...       0.036056       0.5\n",
            "std         0.987483       5.473258  ...       0.921006       0.5\n",
            "min        -1.996583     -56.407510  ...      -0.353229       0.0\n",
            "25%        -0.987214      -2.750573  ...      -0.342914       0.0\n",
            "50%        -0.345333      -0.732524  ...      -0.262513       0.5\n",
            "75%         0.800652       1.035700  ...       0.046539       1.0\n",
            "max         1.642058       2.454930  ...     102.362243       1.0\n",
            "\n",
            "[8 rows x 31 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## 3) Apply Decision Tree Classifier Model and Evaluate the Performance with Confusion matrix, accuracy, precision, Recall and F1 Score."
      ],
      "metadata": {
        "id": "r_DRKk24dLd9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab379e96"
      },
      "source": [
        "# Task\n",
        "Split the `df_resampled` DataFrame into features (X) and target (y), and then further divide them into training and testing sets using `train_test_split`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2c8be0b"
      },
      "source": [
        "## Prepare Data for Model Training\n",
        "\n",
        "### Subtask:\n",
        "Split the resampled dataset (df_resampled) into features (X) and target (y), and then further divide them into training and testing sets using `train_test_split`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7346d15"
      },
      "source": [
        "### Reasoning:\n",
        "Before applying any machine learning model, it's crucial to prepare the data. This involves two main steps:\n",
        "1. **Separating Features and Target**: The dataset needs to be split into features (input variables, typically denoted as `X`) and the target variable (output variable, typically denoted as `y`). This distinction is fundamental for supervised learning tasks.\n",
        "2. **Splitting into Training and Testing Sets**: To evaluate the model's performance on unseen data and prevent overfitting, the dataset is further divided into training and testing sets. The training set is used to train the model, while the testing set is used to assess its generalization capability. A `random_state` is set for reproducibility, ensuring that the split is the same every time the code is run."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4a0c469"
      },
      "source": [
        "**Reasoning**:\n",
        "I will separate the features and target variable from the `df_resampled` DataFrame, then split them into training and testing sets using `train_test_split`, and finally print the shapes of the resulting datasets to verify the split.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "a5e57945",
        "outputId": "0cae0bfc-0805-4cdc-ca0a-e07361463d7f"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = df_resampled.drop('Class', axis=1)\n",
        "y = df_resampled['Class']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print the shapes of the resulting sets\n",
        "print(f\"Shape of X_train: {X_train.shape}\")\n",
        "print(f\"Shape of y_train: {y_train.shape}\")\n",
        "print(f\"Shape of X_test: {X_test.shape}\")\n",
        "print(f\"Shape of y_test: {y_test.shape}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train: (454904, 30)\n",
            "Shape of y_train: (454904,)\n",
            "Shape of X_test: (113726, 30)\n",
            "Shape of y_test: (113726,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d59f90f"
      },
      "source": [
        "## Train Decision Tree Classifier\n",
        "\n",
        "### Subtask:\n",
        "Initialize and train a `DecisionTreeClassifier` on the training data (X_train, y_train).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5d169a0"
      },
      "source": [
        "**Reasoning**:\n",
        "I will import the `DecisionTreeClassifier` from `sklearn.tree`, instantiate it with a `random_state` for reproducibility, and then train the model using the `fit` method on the `X_train` and `y_train` datasets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "206ac3ed",
        "outputId": "11fce125-c764-4bbf-ad82-ae1142d899c7"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Initialize the Decision Tree Classifier\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the model on the training data\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "print(\"Decision Tree Classifier trained successfully.\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Classifier trained successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b01490a1"
      },
      "source": [
        "**Reasoning**:\n",
        "The Decision Tree Classifier has been trained. The next step is to use this trained model to make predictions on the test dataset (`X_test`) to prepare for evaluating its performance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "de446030",
        "outputId": "3403c327-4c5b-49fc-97f9-a1c4a9f7628a"
      },
      "source": [
        "y_pred = dt_classifier.predict(X_test)\n",
        "print(\"Predictions made on the test set.\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions made on the test set.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "918e0d6b"
      },
      "source": [
        "**Reasoning**:\n",
        "To evaluate the performance of the trained Decision Tree Classifier, I will calculate and display common classification metrics such as the Confusion Matrix, Accuracy, Precision, Recall, and F1-Score. This will provide a comprehensive understanding of the model's performance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "d8d80587",
        "outputId": "8a2098c1-c0e5-4506-faf7-ea0e204cc837"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            " [[56610   140]\n",
            " [   56 56920]]\n",
            "\n",
            "Accuracy: 0.9983\n",
            "Precision: 0.9975\n",
            "Recall: 0.9990\n",
            "F1 Score: 0.9983\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fef4897f"
      },
      "source": [
        "The Decision Tree Classifier was successfully applied and evaluated. The model demonstrated strong performance in classifying fraudulent transactions.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The dataset was successfully split into training and testing sets, with 80% used for training (454,904 samples) and 20% for testing (113,726 samples).\n",
        "*   A Decision Tree Classifier was initialized and trained on the resampled training data.\n",
        "*   The model achieved a high **Accuracy** of 0.9983 on the test set.\n",
        "*   The **Precision** for the positive class (fraudulent transactions) was 0.9975, indicating a very low rate of false positives.\n",
        "*   The **Recall** for the positive class was 0.9990, meaning the model correctly identified nearly all actual fraudulent transactions.\n",
        "*   The **F1 Score** was 0.9983, representing a balanced measure of precision and recall.\n",
        "*   The confusion matrix shows that out of 113,726 test samples, there were only 140 false positives and 56 false negatives, demonstrating excellent classification performance.\n",
        "\n"
      ]
    }
  ]
}